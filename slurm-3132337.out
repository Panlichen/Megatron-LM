/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launch.py:183: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
[2024-10-19 17:29:38,664] torch.distributed.run: [WARNING] 
[2024-10-19 17:29:38,664] torch.distributed.run: [WARNING] *****************************************
[2024-10-19 17:29:38,664] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-10-19 17:29:38,664] torch.distributed.run: [WARNING] *****************************************
using world size: 8, data-parallel size: 2, context-parallel size: 1, tensor-model-parallel size: 2, encoder-tensor-model-parallel size: 0, pipeline-model-parallel size: 2, encoder-pipeline-model-parallel size: 0
WARNING: Please specify --split when using --data-path. Using legacy default value of "969, 30, 1"
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
using torch.float32 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['codeparrot_content_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 12
  encoder_pipeline_model_parallel_size ............ 0
  encoder_seq_length .............................. 1024
  encoder_tensor_model_parallel_size .............. 0
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 200
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_model_parallel_size ...................... 1
  ffn_hidden_size ................................. 3072
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 288
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 768
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 64
  lazy_mpu_init ................................... None
  load ............................................ /HOME/scz1075/run/Megatron-LM/experiments/codeparrot-small
  local_rank ...................................... 0
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0005
  lr_decay_iters .................................. 150000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 2000
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... gpt2-merges.txt
  micro_batch_size ................................ 18
  min_loss_scale .................................. 1.0
  min_lr .......................................... 0.0
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_z_loss_coeff ................................ None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 12
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_layers ...................................... 12
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float32
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 2
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  qk_layernorm .................................... False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  renormalize_blend_weights ....................... False
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 10000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ /HOME/scz1075/run/Megatron-LM/experiments/codeparrot-small
  save_interval ................................... 2000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 969, 30, 1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. experiments/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 3
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 2
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_legacy_models ............................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tp_pp_dp_mapping ............................ False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... gpt2-vocab.json
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 8
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
> initializing torch distributed ...
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 2
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory `/data/run01/scz1075/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for `default'.
make: Leaving directory `/data/run01/scz1075/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.071 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 0.692 seconds
Parameters: type(args): <class 'argparse.Namespace'>
num_layers: 12
encoder_num_layers: 12
decoder_num_layers: None
hidden_size: 768
ffn_hidden_size: 3072
num_attention_heads: 12
kv_channels: 64
group_query_attention: False
num_query_groups: 1
max_position_embeddings: 1024
position_embedding_type: learned_absolute
use_rotary_position_embeddings: False
rotary_base: 10000
rotary_percent: 1.0
rotary_interleaved: False
rotary_seq_len_interpolation_factor: None
add_position_embedding: True
make_vocab_size_divisible_by: 128
normalization: LayerNorm
norm_epsilon: 1e-05
apply_layernorm_1p: False
apply_residual_connection_post_layernorm: False
openai_gelu: False
squared_relu: False
swiglu: False
onnx_safe: None
bert_binary_head: True
untie_embeddings_and_output_weights: False
attention_dropout: 0.1
hidden_dropout: 0.1
weight_decay: 0.1
start_weight_decay: 0.1
end_weight_decay: 0.1
weight_decay_incr_style: constant
clip_grad: 1.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_eps: 1e-08
sgd_momentum: 0.9
micro_batch_size: 18
global_batch_size: 288
rampup_batch_size: None
decrease_batch_size_if_needed: False
recompute_granularity: None
check_for_nan_in_loss_and_grad: True
distribute_saved_activations: False
recompute_method: None
recompute_num_layers: None
clone_scatter_output_in_embedding: True
profile: False
profile_step_start: 10
profile_step_end: 12
profile_ranks: [0]
tp_comm_overlap: False
tp_comm_overlap_cfg: None
tp_comm_overlap_ag: True
tp_comm_overlap_rs: True
tp_comm_overlap_rs_dgrad: False
[rank6]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
tp_comm_bulk_dgrad: True[rank3]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank1]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
tp_comm_bulk_wgrad: True[rank5]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

[rank4]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
use_cpu_initialization: None[rank2]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())

empty_unused_memory_level: 0
[rank7]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
deterministic_mode: False
check_weight_hash_across_dp_replicas_interval: None
calculate_per_token_loss: False
train_sync_interval: None
train_iters: 3
train_samples: None
log_interval: 1
exit_interval: None
exit_duration_in_mins: None
exit_signal_handler: False
tensorboard_dir: experiments/tensorboard
masked_softmax_fusion: True
bias_gelu_fusion: True
bias_swiglu_fusion: True
bias_dropout_fusion: True
apply_rope_fusion: True
cross_entropy_loss_fusion: False
use_flash_attn: False
add_bias_linear: True
add_qkv_bias: False
optimizer: adam
dataloader_type: single
async_tensor_model_parallel_allreduce: False
no_persist_layer_norm: False
sequence_parallel: False
gradient_accumulation_fusion: True
deprecated_use_mcore_models: False
use_legacy_models: False
manual_gc: False
manual_gc_interval: 0
manual_gc_eval: True
tp_comm_split_ag: True
tp_comm_split_rs: True
seed: 1234
data_parallel_random_init: False
init_method_std: 0.02
init_method_xavier_uniform: False
lr: 0.0005
lr_decay_style: cosine
lr_wsd_decay_style: exponential
lr_decay_iters: 150000
lr_decay_samples: None
lr_wsd_decay_samples: None
lr_wsd_decay_iters: None
lr_warmup_fraction: None
lr_warmup_iters: 2000
lr_warmup_samples: 0
lr_warmup_init: 0.0
min_lr: 0.0
override_opt_param_scheduler: False
use_checkpoint_opt_param_scheduler: False
decoupled_lr: None
decoupled_min_lr: None
save: /HOME/scz1075/run/Megatron-LM/experiments/codeparrot-small
save_interval: 2000
no_save_optim: None
no_save_rng: None
load: /HOME/scz1075/run/Megatron-LM/experiments/codeparrot-small
no_load_optim: None
no_load_rng: None
non_persistent_save_interval: None
non_persistent_ckpt_type: None
non_persistent_global_ckpt_dir: None
finetune: False
pretrained_checkpoint: None
ckpt_step: None
perform_initialization: True
use_checkpoint_args: False
exit_on_missing_checkpoint: False
use_dist_ckpt_deprecated: False
auto_detect_ckpt_format: False
dist_ckpt_format_deprecated: None
ckpt_format: torch_dist
ckpt_convert_format: None
ckpt_convert_save: None
ckpt_convert_update_legacy_dist_opt_format: False
ckpt_fully_parallel_save_deprecated: False
ckpt_fully_parallel_save: True
async_save: None
ckpt_fully_parallel_load: False
ckpt_assume_constant_structure: False
dist_ckpt_strictness: assume_ok_unexpected
fp16: False
bf16: False
loss_scale: None
initial_loss_scale: 4294967296
min_loss_scale: 1.0
loss_scale_window: 1000
hysteresis: 2
fp32_residual_connection: False
apply_query_key_layer_scaling: False
attention_softmax_in_fp32: False
accumulate_allreduce_grads_in_fp32: False
fp16_lm_cross_entropy: False
tensor_model_parallel_size: 2
encoder_tensor_model_parallel_size: 0
pipeline_model_parallel_size: 2
encoder_pipeline_model_parallel_size: 0
pipeline_model_parallel_split_rank: None
num_layers_per_virtual_pipeline_stage: None
overlap_p2p_comm: False
distributed_backend: nccl
distributed_timeout_minutes: 10
overlap_grad_reduce: False
defer_embedding_wgrad_compute: False
wgrad_deferral_limit: 0
align_grad_reduce: True
ddp_bucket_size: None
ddp_average_in_collective: False
overlap_param_gather: False
overlap_param_gather_with_optimizer_step: False
align_param_gather: False
scatter_gather_tensors_in_pipeline: True
use_ring_exchange_p2p: False
local_rank: 0
lazy_mpu_init: None
standalone_embedding_stage: False
use_distributed_optimizer: False
context_parallel_size: 1
nccl_communicator_config_path: None
use_tp_pp_dp_mapping: False
eval_iters: 10
eval_interval: 200
test_mode: False
skip_train: False
data_path: ['codeparrot_content_document']
renormalize_blend_weights: False
split: 969, 30, 1
train_data_path: None
valid_data_path: None
test_data_path: None
data_cache_path: None
mmap_bin_files: True
mock_data: False
vocab_size: None
vocab_file: gpt2-vocab.json
merge_file: gpt2-merges.txt
vocab_extra_ids: 0
seq_length: 1024
encoder_seq_length: 1024
decoder_seq_length: None
retriever_seq_length: 256
sample_rate: 1.0
mask_prob: 0.15
short_seq_prob: 0.1
num_workers: 2
tokenizer_type: GPT2BPETokenizer
tokenizer_model: None
tiktoken_pattern: None
tiktoken_num_special_tokens: 1000
tiktoken_special_tokens: None
reset_position_ids: False
reset_attention_mask: False
eod_mask_loss: False
create_attention_mask_in_dataloader: True
num_dataset_builder_threads: 1
s3_cache_path: None
adlr_autoresume: False
adlr_autoresume_interval: 1000
ict_head_size: None
biencoder_projection_dim: 0
biencoder_shared_query_context_model: False
ict_load: None
bert_load: None
titles_data_path: None
query_in_block_prob: 0.1
use_one_sent_docs: False
evidence_data_path: None
retriever_report_topk_accuracies: []
retriever_score_scaling: False
block_data_path: None
embedding_path: None
indexer_batch_size: 128
indexer_log_interval: 1000
num_classes: 1000
img_h: 224
img_w: 224
num_channels: 3
patch_dim: 16
classes_fraction: 1.0
data_per_class_fraction: 1.0
data_sharding: True
head_lr_mult: 1.0
vision_pretraining: False
vision_pretraining_type: classify
vision_backbone_type: vit
swin_backbone_type: tiny
mask_type: random
mask_factor: 1.0
iter_per_epoch: 1250
dino_local_img_size: 96
dino_local_crops_number: 10
dino_head_hidden_size: 2048
dino_bottleneck_size: 256
dino_freeze_last_layer: 1
dino_norm_last_layer: False
dino_warmup_teacher_temp: 0.04
dino_teacher_temp: 0.07
dino_warmup_teacher_temp_epochs: 30
qk_layernorm: False
expert_model_parallel_size: 1
num_experts: None
moe_router_load_balancing_type: aux_loss
moe_router_topk: 2
moe_router_pre_softmax: False
moe_grouped_gemm: False
moe_aux_loss_coeff: 0.0
moe_z_loss_coeff: None
moe_input_jitter_eps: None
moe_token_dispatcher_type: allgather
moe_per_layer_logging: False
moe_expert_capacity_factor: None
moe_pad_expert_input_to_capacity: False
moe_token_drop_policy: probs
moe_layer_recompute: False
moe_extended_tp: False
log_params_norm: False
log_num_zeros_in_grad: False
log_throughput: False
log_progress: False
timing_log_level: 0
barrier_with_L1_time: True
timing_log_option: minmax
tensorboard_log_interval: 1
tensorboard_queue_size: 1000
log_timers_to_tensorboard: False
log_loss_scale_to_tensorboard: True
log_validation_ppl_to_tensorboard: False
log_memory_to_tensorboard: False
log_world_size_to_tensorboard: False
wandb_project: 
wandb_exp_name: 
wandb_save_dir: 
logging_level: None
log_straggler: False
disable_straggler_on_startup: False
straggler_ctrlr_port: 65535
straggler_minmax_count: 1
inference_batch_times_seqlen_threshold: 512
max_tokens_to_oom: 12000
output_bert_embeddings: False
bert_embedder_type: megatron
fp8: None
fp8_margin: 0
fp8_interval: 1
fp8_amax_history_len: 1
fp8_amax_compute_algo: most_recent
fp8_wgrad: True
transformer_impl: transformer_engine
retro_project_dir: None
retro_add_retriever: False
retro_cyclic_train_iters: None
retro_encoder_layers: 2
retro_encoder_hidden_dropout: 0.1
retro_encoder_attention_dropout: 0.1
retro_num_neighbors: 2
retro_num_retrieved_chunks: 2
retro_attention_gate: 1
retro_verify_neighbor_count: True
spec: None
hybrid_attention_ratio: 0.0
hybrid_mlp_ratio: 0.0
hybrid_override_pattern: None
yaml_cfg: None
enable_one_logger: True
one_logger_project: megatron-lm
one_logger_run_name: None
one_logger_async: False
app_tag_run_name: None
app_tag_run_version: 0.0.0
enable_ft_package: False
config_logger_dir: 
rank: 0
world_size: 8
use_dist_ckpt: True
transformer_pipeline_model_parallel_size: 2
data_parallel_size: 2
virtual_pipeline_model_parallel_size: None
params_dtype: torch.float32
consumed_train_samples: 0
skipped_train_samples: 0
consumed_valid_samples: 0
variable_seq_lengths: False
padded_vocab_size: 50432
Parameters Done
[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 8.211
[after megatron is initialized] datetime: 2024-10-19 17:29:56 
building GPT model ...
use_te: True, HAVE_TE: True
use_te: True, HAVE_TE: Trueuse_te: True, HAVE_TE: Trueuse_te: True, HAVE_TE: Trueuse_te: True, HAVE_TE: Trueuse_te: True, HAVE_TE: Trueuse_te: True, HAVE_TE: True


use_te: True, HAVE_TE: True



/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/attention.py:3051: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/transformer_engine/pytorch/module/base.py:710: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
  warnings.warn(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 40644864 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 41429760

wrap_with_ddp: True
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 40644864
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 41429760
WARNING: could not find the metadata file /HOME/scz1075/run/Megatron-LM/experiments/codeparrot-small/latest_checkpointed_iteration.txt
    will not load any checkpoints and will start from random
(min, max) time across ranks (ms):
    load-checkpoint ................................: (1.13, 1.16)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-10-19 17:29:56 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      864
    validation: 2880
    test:       2880
> building train, validation, and test datasets for GPT ...
Traceback (most recent call last):
  File "/data/run01/scz1075/Megatron-LM/pretrain_gpt.py", line 248, in <module>
    pretrain(
  File "/data/run01/scz1075/Megatron-LM/megatron/training/training.py", line 304, in pretrain
    = build_train_valid_test_data_iterators(
  File "/data/run01/scz1075/Megatron-LM/megatron/training/training.py", line 1656, in build_train_valid_test_data_iterators
    build_train_valid_test_data_loaders(
  File "/data/run01/scz1075/Megatron-LM/megatron/training/training.py", line 1617, in build_train_valid_test_data_loaders
    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
  File "/data/run01/scz1075/Megatron-LM/megatron/training/training.py", line 1587, in build_train_valid_test_datasets
    return build_train_valid_test_datasets_provider(train_valid_test_num_samples)
  File "/data/run01/scz1075/Megatron-LM/pretrain_gpt.py", line 236, in train_valid_test_datasets_provider
    ).build()
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 126, in build
    datasets = self._build_blended_dataset_splits()
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 191, in _build_blended_dataset_splits
    return self._build_megatron_dataset_splits(prefixes[0], split, self.sizes)
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 437, in _build_megatron_dataset_splits
    self.build_generic_dataset(
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/blended_megatron_dataset_builder.py", line 485, in build_generic_dataset
    dataset = cls(*args)
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/gpt_dataset.py", line 112, in __init__
    self._build_document_sample_shuffle_indices()
  File "/data/run01/scz1075/Megatron-LM/megatron/core/datasets/gpt_dataset.py", line 420, in _build_document_sample_shuffle_indices
    from megatron.core.datasets import helpers
ImportError: /lib64/libc.so.6: version `GLIBC_2.32' not found (required by /data/run01/scz1075/Megatron-LM/megatron/core/datasets/helpers.cpython-310-x86_64-linux-gnu.so)
[2024-10-19 17:30:03,704] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23156 closing signal SIGTERM
[2024-10-19 17:30:03,704] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23157 closing signal SIGTERM
[2024-10-19 17:30:03,706] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23158 closing signal SIGTERM
[2024-10-19 17:30:03,707] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23159 closing signal SIGTERM
[2024-10-19 17:30:03,710] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23160 closing signal SIGTERM
[2024-10-19 17:30:03,724] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23161 closing signal SIGTERM
[2024-10-19 17:30:03,743] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 23162 closing signal SIGTERM
[2024-10-19 17:30:04,244] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 23155) of binary: /HOME/scz1075/.conda/envs/torch221_cuda121/bin/python3
Traceback (most recent call last):
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launch.py", line 198, in <module>
    main()
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launch.py", line 194, in main
    launch(args)
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launch.py", line 179, in launch
    run(args)
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/HOME/scz1075/.conda/envs/torch221_cuda121/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-19_17:30:03
  host      : g0023.para.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 23155)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
